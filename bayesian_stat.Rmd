---
title: "Bayesian Statistics"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Bayesian Statistics

## Probability

* Complement : P(Ac) = 1−P(A)
* P(A union B) = P(A) + P(B) − P(A inter B)
* If A mutually exclusive : p(union Ai) = sum(p(Ai))
* Odds = p(A) / (1-p(A))
* Expectation : E(X)=sum(xi*p(X=xi))
* Classical framework : outcomes that are equally likely have equal probabilities => works really well when we have equally likely outcomes or well-defined equally likely outcomes. That's very difficult to apply in any of these other cases.
* Frequentist framework : requires us to have a hypothetical infinite sequence of events, and then we look at the relevant frequency, in that hypothetical infinite sequence => works great when we can define a hypothetical infinite sequence. But then we can ask other questions, and they become more complicated under this approach.For example, what is the probability that it rains tomorrow? In this case, we need to think about hypothetical infinite sequence of tomorrow, and see what fraction of these infinite possible tomorrows have rain, which is a bit strange to think about.
* Bayesian probability : your probability represents your own perspective, it's your measure of uncertainty, and it takes into account what you know about a particular problem => inherently a subjective approach to probability, but it can work well in a mathematically rigorous foundation, and it leads to much more intuitive results in many cases than the Frequentist approach

## Bayes’ theorem

### Conditional probability

* p(A | B) = p(A inter B) / p(B)
* Independance : 
    + p(A | B) = p(A) 
    + p(A inter B) = p(A)*p(B)
* If p(A | B) <> p(A) => A and B not independant ! 

### Bayes’ theorem

* Bayes' Theorem is used to reverse the direction of conditioning. Suppose we want to ask what's the P(A|B) but we know it in terms of P(B|A). So we can write the P(A|B) = P(B|A) P(A) / P(B) = P(B|A) P(A) / P(B|A) P(A) + P(B| not A) P(not A) This does work out back to be the same as the P(A and B) / P(B)
* Used if not all info to compute directly p(A inter B) / p(B)
* Results sometimes surprising, e.g. rare diseases => even with accurate tests, false positives > true positives
* Example :
    + p(+|HIV) = 0.977
    + p(-|noHIV) = 0.926
    + p(HIV) = 0.0026
* This obviously has important policy implications for things like mandatory testing. It makes much more sense to test in a sub population where the prevalence of HIV is higher, rather than in a general population where it's quite rare
* To conclude, Bayes' Theorem is an important part of our approach to Bayesian statistics. We can use it to update our information. We start with prior beliefs, we'll collect data, we'll then condition on the data to lead to our posterior beliefs. Bayes' Theorem is the coherent way to do this updating

```{r hiv, eval=F}
p_pos_HIV<- 0.977
p_neg_noHIV<-0.926
p_HIV<- 0.0026

p_HIV_pos<-(p_pos_HIV*p_HIV)/(p_pos_HIV*p_HIV+(1-p_neg_noHIV)*(1-p_HIV))
print(proba)
p_HIV_pos

```

## Review of distributions

### Bernoulli and binomial distributions

* Bernoulli distribution : used when we have two possible outcomes, such as flipping a coin
* The generalization of the Bernoulli when we have n repeated trials is a binomial.Binomial is just the sum of the N independent Bernoullis. We can say X follows a binomial distribution with parameters n and p

### Uniform distribution

* The PDF is sort of proportional to the probability that the random variable will take a particular value. In differential calculus sense because it can take an infinite number of possible values. The key idea is that if you integrate the PDF over an interval, it gives you the probability that the random variable would be in that interval
* uniform (0,1) : f(x) = I{0<=x<=1} => proba = integrating f(x)
* uniform (a,b) : f(x) = 1/(b-a) * I{a<=x<=b} 

### Exponential and normal distributions

* Exponential distr : events that are occurring at a particular rate, and the exponential is the waiting time in between events. For example, you're waiting for a bus. The bus comes once every ten minutes. And then, the exponential is your waiting time. This is particularly true for events coming from a poisson process

```{r distrib, eval=F}
## Quizz 2
# titanic
s<-203+118+178+212
d<-122+167+528+673

(203+122)/(s+d)
s/(s+d)
(203/(s+d))/((203+122)/(s+d))

# marbles
pblue<-1/3*3/5+1/3*1/6+1/3*0
3/5*1/3/pblue

pred<-1-pblue
1*1/3/pred

## Quizz 3
4*3*2*1/(3*2*1)
0.2*1+0.2*2+0.1*3
(1-0.2)**3

## Quizz honor
lambda<-3
x<-0.3
1-(1-exp(-lambda*x))
34+2*(-5-1*(-2))
```


# Statistical Inference

## Frequentist inference

* Under the frequentist paradigm, you view the data as a random sample from some larger, potentially hypothetical population. We can then make probability statements i.e, long-run frequency statements based on this larger population

### Confidence intervals

* Under the frequentist paradigm, what this means is we have to think back to our infinite hypothetical sequence of events. So if we were to repeat this trial an infinite number of times, or an arbitrary large number of times.
* Each time we create a confidence interval in this way based on the data we observe. Than on average 95% of the intervals we make will contain the true value of p
* What's the probability that this interval contains a true p? Well, we don't know for this particular interval. But under the frequentist paradigm, we're assuming that there is a fixed right answer for p. Either p is in that interval or it's not in that interval. And so technically, from a frequentist perspective, the probability that p is in this interval is either 0 or 1. This is not a particularly satisfying explanation. In the other hand when we get to the Bayesian approach we will be able to compute an interval and actually say there is probably a p is in this interval is 95% based on a random interpretation of an unknown parameter

### Likelihood function, max likelihood

* The likelihood function is this density function thought of as a function of theta. This is not a probability distribution anymore, but it is still a function for theta.One way to estimate theta is that we choose the theta that gives us the largest value of the likelihood. It makes the data the most likely to occur for the particular data we observed.This is referred to as the maximum likelihood estimate, or MLE, maximum likelihood estimate.
* In practice, it's usually easier to maximize the natural logarithm of the likelihood, referred to as the log likelihood.

### Computing the MLE

* Maximum likelihood estimators have many desirable mathematical properties. They're unbiased, they're consistent, and they're invariant.
* In general, under certain regularity conditions, we can say that the MLE is approximately normally distributed with mean at the true value of theta and variance one over the Fisher information the value at theta hat. We'll return to the Fisher information later. The Fisher information is a measure of how much information about theta is in each data point.

```{r freq_inf, eval=F}
## Quizz 4
theta<-47/100
n<-100
up<-theta+1.96*(theta*(1-theta)/n)**(1/2)
low<-theta-1.96*(theta*(1-theta)/n)**(1/2)
5/(2+2.5+4.1+1.8+4.0)
mean(c(−1.2,0.5,0.8,−0.3))

```
```{r likelihood_fct}
## fonction likelihood
y<-72
n<-400
lik_fct<-function(n,y,theta){return(theta^y*(1-theta)^(n-y))}
theta<-seq(from=0.01, to=0.99, by=0.01)
plot(theta, lik_fct(n,y, theta))
abline(v=y/n)


```

## Bayesian inference

### Inference examples : frequentist, bayesian
### Continuous version of Bayesian theorem
### Posterior intervals








